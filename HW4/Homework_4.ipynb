{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5Jm3UN_Hfsu"
   },
   "source": [
    "## **Homework 4**\n",
    "**Instructions**\n",
    "* This homework focuses on understanding and applying CoCoOp for CLIP prompt tuning. It consists of **four questions** designed to assess both theoretical understanding and practical application.\n",
    "\n",
    "* Please organize your answers and results for the questions below and submit this jupyter notebook as **a .pdf file**.\n",
    "\n",
    "* **Deadline: 11/26 (Sat) 23:59**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeRABv42Ku4E"
   },
   "source": [
    "### **Preparation**\n",
    "\n",
    "* Run the code below before proceeding with the homework (Q1, Q2).\n",
    "* If an error occurs, click ‘Run Session Again’ and then restart the runtime from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jNOsgBEzKucv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ProMetaR' already exists and is not an empty directory.\n",
      "/Users/and___young/Documents/@24-2/DeepLearning/COSE474_NayoungKim/HW4/ProMetaR\n",
      "fatal: destination path 'Dassl.pytorch' already exists and is not an empty directory.\n",
      "/Users/and___young/Documents/@24-2/DeepLearning/COSE474_NayoungKim/HW4/ProMetaR/Dassl.pytorch\n",
      "Requirement already satisfied: flake8==3.7.9 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (3.7.9)\n",
      "Requirement already satisfied: yapf==0.29.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (0.29.0)\n",
      "Requirement already satisfied: isort==4.3.21 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (4.3.21)\n",
      "Requirement already satisfied: yacs in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (0.1.8)\n",
      "Requirement already satisfied: gdown in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (5.2.0)\n",
      "Requirement already satisfied: tb-nightly in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (2.19.0a20241121)\n",
      "Requirement already satisfied: future in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (1.0.0)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (1.5.2)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (4.66.1)\n",
      "Requirement already satisfied: ftfy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (6.1.1)\n",
      "Requirement already satisfied: regex in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 12)) (2024.11.6)\n",
      "Requirement already satisfied: wilds==1.2.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 13)) (1.2.2)\n",
      "Requirement already satisfied: tabulate in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 14)) (0.9.0)\n",
      "Requirement already satisfied: entrypoints<0.4.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from flake8==3.7.9->-r requirements.txt (line 1)) (0.3)\n",
      "Requirement already satisfied: pyflakes<2.2.0,>=2.1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from flake8==3.7.9->-r requirements.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: pycodestyle<2.6.0,>=2.5.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from flake8==3.7.9->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from flake8==3.7.9->-r requirements.txt (line 1)) (0.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (1.26.4)\n",
      "Requirement already satisfied: ogb>=1.2.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (1.3.6)\n",
      "Requirement already satisfied: outdated>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (0.2.2)\n",
      "Requirement already satisfied: pandas>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (2.0.3)\n",
      "Requirement already satisfied: pillow>=7.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (10.1.0)\n",
      "Requirement already satisfied: pytz>=2020.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (2023.3.post1)\n",
      "Requirement already satisfied: torch>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (1.11.0)\n",
      "Requirement already satisfied: torchvision>=0.8.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (0.12.0)\n",
      "Requirement already satisfied: PyYAML in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from yacs->-r requirements.txt (line 4)) (6.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gdown->-r requirements.txt (line 5)) (4.12.2)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gdown->-r requirements.txt (line 5)) (3.6.0)\n",
      "Requirement already satisfied: requests[socks] in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gdown->-r requirements.txt (line 5)) (2.31.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tb-nightly->-r requirements.txt (line 6)) (2.0.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tb-nightly->-r requirements.txt (line 6)) (1.59.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tb-nightly->-r requirements.txt (line 6)) (3.5.1)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tb-nightly->-r requirements.txt (line 6)) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tb-nightly->-r requirements.txt (line 6)) (3.19.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tb-nightly->-r requirements.txt (line 6)) (58.1.0)\n",
      "Requirement already satisfied: six>1.9 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tb-nightly->-r requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tb-nightly->-r requirements.txt (line 6)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tb-nightly->-r requirements.txt (line 6)) (3.0.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 9)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 9)) (3.5.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from ftfy->-r requirements.txt (line 11)) (0.2.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from markdown>=2.6.8->tb-nightly->-r requirements.txt (line 6)) (6.6.0)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from ogb>=1.2.6->wilds==1.2.2->-r requirements.txt (line 13)) (1.26.15)\n",
      "Requirement already satisfied: littleutils in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from outdated>=0.2.0->wilds==1.2.2->-r requirements.txt (line 13)) (0.2.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas>=1.1.0->wilds==1.2.2->-r requirements.txt (line 13)) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas>=1.1.0->wilds==1.2.2->-r requirements.txt (line 13)) (2023.3)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (4.6.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from werkzeug>=1.0.1->tb-nightly->-r requirements.txt (line 6)) (2.1.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from beautifulsoup4->gdown->-r requirements.txt (line 5)) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (2024.8.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (1.7.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tb-nightly->-r requirements.txt (line 6)) (3.15.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "/Users/and___young/Documents/@24-2/DeepLearning/COSE474_NayoungKim/HW4/ProMetaR\n",
      "Requirement already satisfied: ftfy==6.1.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (6.1.1)\n",
      "Requirement already satisfied: regex in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (4.66.1)\n",
      "Requirement already satisfied: learn2learn==0.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from ftfy==6.1.1->-r requirements.txt (line 1)) (0.2.6)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (1.26.4)\n",
      "Requirement already satisfied: gym>=0.14.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (0.26.2)\n",
      "Requirement already satisfied: torch>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (1.11.0)\n",
      "Requirement already satisfied: torchvision>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (0.12.0)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (1.10.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (2.31.0)\n",
      "Requirement already satisfied: gsutil in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (5.31)\n",
      "Requirement already satisfied: qpth>=0.0.15 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (0.0.18)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gym>=0.14.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.1.0)\n",
      "Requirement already satisfied: importlib_metadata>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gym>=0.14.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (6.6.0)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gym>=0.14.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.0.8)\n",
      "Requirement already satisfied: cvxpy>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.6.0)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (4.6.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torchvision>=0.3.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (10.1.0)\n",
      "Requirement already satisfied: argcomplete>=1.9.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.5.1)\n",
      "Requirement already satisfied: crcmod>=1.7 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.7)\n",
      "Requirement already satisfied: fasteners>=0.14.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.19)\n",
      "Requirement already satisfied: gcs-oauth2-boto-plugin>=3.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.2)\n",
      "Requirement already satisfied: google-apitools>=0.5.32 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.5.32)\n",
      "Requirement already satisfied: httplib2==0.20.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.20.4)\n",
      "Requirement already satisfied: google-reauth>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.1.1)\n",
      "Requirement already satisfied: monotonic>=1.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.6)\n",
      "Requirement already satisfied: pyOpenSSL>=0.13 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (24.2.1)\n",
      "Requirement already satisfied: retry-decorator>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.1.1)\n",
      "Requirement already satisfied: six>=1.16.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: google-auth==2.17.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.17.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (4.7.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0dev,>=3.6.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.11.7)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from httplib2==0.20.4->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->learn2learn==0.2.0->-r requirements.txt (line 4)) (2024.8.30)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: osqp>=0.6.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.6.7.post3)\n",
      "Requirement already satisfied: clarabel>=0.5.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.9.0)\n",
      "Requirement already satisfied: scs>=3.2.4.post1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.2.7)\n",
      "Requirement already satisfied: boto>=2.29.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gcs-oauth2-boto-plugin>=3.2->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.49.0)\n",
      "Requirement already satisfied: oauth2client>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gcs-oauth2-boto-plugin>=3.2->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (4.1.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from rsa<5,>=3.1.4->google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.5.1)\n",
      "Requirement already satisfied: pyu2f in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from google-reauth>=0.1.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.1.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from importlib_metadata>=4.8.0->gym>=0.14.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.15.0)\n",
      "Requirement already satisfied: cryptography<44,>=41.0.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pyOpenSSL>=0.13->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (43.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.18.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from cryptography<44,>=41.0.5->pyOpenSSL>=0.13->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.15.1)\n",
      "Requirement already satisfied: qdldl in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from osqp>=0.6.2->cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.1.7.post4)\n",
      "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from cffi>=1.12->cryptography<44,>=41.0.5->pyOpenSSL>=0.13->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.21)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "mkdir: outputs: File exists\n",
      "mkdir: data: File exists\n",
      "/Users/and___young/Documents/@24-2/DeepLearning/COSE474_NayoungKim/HW4/ProMetaR/data\n",
      "mkdir: eurosat: File exists\n",
      "zsh:1: command not found: wget\n",
      "unzip:  cannot find or open EuroSAT.zip, EuroSAT.zip.zip or EuroSAT.zip.ZIP.\n",
      "/Users/and___young/Documents/@24-2/DeepLearning/COSE474_NayoungKim/HW4/ProMetaR/data/eurosat\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Ip7yaCWFi0eaOFUGga0lUdVi_DDQth1o\n",
      "To: /Users/and___young/Documents/@24-2/DeepLearning/COSE474_NayoungKim/HW4/ProMetaR/data/eurosat/split_zhou_EuroSAT.json\n",
      "100%|██████████████████████████████████████| 3.01M/3.01M [00:00<00:00, 10.7MB/s]\n",
      "/Users/and___young/Documents/@24-2/DeepLearning/COSE474_NayoungKim/HW4/ProMetaR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/and___young/Documents/@24-2/DeepLearning/COSE474_NayoungKim/HW4/ProMetaR/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended\n",
      "  warnings.warn(\"PyTorch version 1.7.1 or higher is recommended\")\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mlvlab/ProMetaR.git\n",
    "%cd ProMetaR/\n",
    "\n",
    "!git clone https://github.com/KaiyangZhou/Dassl.pytorch.git\n",
    "%cd Dassl.pytorch/\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -r requirements.txt\n",
    "!cp -r dassl ../\n",
    "# Install this library (no need to re-build if the source code is modified)\n",
    "# !python setup.py develop\n",
    "%cd ..\n",
    "\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "%mkdir outputs\n",
    "%mkdir data\n",
    "\n",
    "%cd data\n",
    "%mkdir eurosat\n",
    "!wget http://madm.dfki.de/files/sentinel/EuroSAT.zip EuroSAT.zip\n",
    "\n",
    "!unzip -o EuroSAT.zip -d eurosat/\n",
    "%cd eurosat\n",
    "!gdown 1Ip7yaCWFi0eaOFUGga0lUdVi_DDQth1o\n",
    "\n",
    "%cd ../../\n",
    "\n",
    "import os.path as osp\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from clip import clip\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import argparse\n",
    "from dassl.utils import setup_logger, set_random_seed, collect_env_info\n",
    "from dassl.config import get_cfg_default\n",
    "from dassl.engine import build_trainer\n",
    "from dassl.engine import TRAINER_REGISTRY, TrainerX\n",
    "from dassl.metrics import compute_accuracy\n",
    "from dassl.utils import load_pretrained_weights, load_checkpoint\n",
    "from dassl.optim import build_optimizer, build_lr_scheduler\n",
    "\n",
    "# custom\n",
    "import datasets.oxford_pets\n",
    "import datasets.oxford_flowers\n",
    "import datasets.fgvc_aircraft\n",
    "import datasets.dtd\n",
    "import datasets.eurosat\n",
    "import datasets.stanford_cars\n",
    "import datasets.food101\n",
    "import datasets.sun397\n",
    "import datasets.caltech101\n",
    "import datasets.ucf101\n",
    "import datasets.imagenet\n",
    "import datasets.imagenet_sketch\n",
    "import datasets.imagenetv2\n",
    "import datasets.imagenet_a\n",
    "import datasets.imagenet_r\n",
    "\n",
    "def print_args(args, cfg):\n",
    "    print(\"***************\")\n",
    "    print(\"** Arguments **\")\n",
    "    print(\"***************\")\n",
    "    optkeys = list(args.__dict__.keys())\n",
    "    optkeys.sort()\n",
    "    for key in optkeys:\n",
    "        print(\"{}: {}\".format(key, args.__dict__[key]))\n",
    "    print(\"************\")\n",
    "    print(\"** Config **\")\n",
    "    print(\"************\")\n",
    "    print(cfg)\n",
    "\n",
    "def reset_cfg(cfg, args):\n",
    "    if args.root:\n",
    "        cfg.DATASET.ROOT = args.root\n",
    "    if args.output_dir:\n",
    "        cfg.OUTPUT_DIR = args.output_dir\n",
    "    if args.seed:\n",
    "        cfg.SEED = args.seed\n",
    "    if args.trainer:\n",
    "        cfg.TRAINER.NAME = args.trainer\n",
    "    cfg.DATASET.NUM_SHOTS = 16\n",
    "    cfg.DATASET.SUBSAMPLE_CLASSES = args.subsample_classes\n",
    "    cfg.DATALOADER.TRAIN_X.BATCH_SIZE = args.train_batch_size\n",
    "    cfg.OPTIM.MAX_EPOCH = args.epoch\n",
    "\n",
    "def extend_cfg(cfg):\n",
    "    \"\"\"\n",
    "    Add new config variables.\n",
    "    \"\"\"\n",
    "    from yacs.config import CfgNode as CN\n",
    "    cfg.TRAINER.COOP = CN()\n",
    "    cfg.TRAINER.COOP.N_CTX = 16  # number of context vectors\n",
    "    cfg.TRAINER.COOP.CSC = False  # class-specific context\n",
    "    cfg.TRAINER.COOP.CTX_INIT = \"\"  # initialization words\n",
    "    cfg.TRAINER.COOP.PREC = \"fp16\"  # fp16, fp32, amp\n",
    "    cfg.TRAINER.COOP.CLASS_TOKEN_POSITION = \"end\"  # 'middle' or 'end' or 'front'\n",
    "    cfg.TRAINER.COCOOP = CN()\n",
    "    cfg.TRAINER.COCOOP.N_CTX = 4  # number of context vectors\n",
    "    cfg.TRAINER.COCOOP.CTX_INIT = \"a photo of a\"  # initialization words\n",
    "    cfg.TRAINER.COCOOP.PREC = \"fp16\"  # fp16, fp32, amp\n",
    "    cfg.TRAINER.PROMETAR = CN()\n",
    "    cfg.TRAINER.PROMETAR.N_CTX_VISION = 4  # number of context vectors at the vision branch\n",
    "    cfg.TRAINER.PROMETAR.N_CTX_TEXT = 4  # number of context vectors at the language branch\n",
    "    cfg.TRAINER.PROMETAR.CTX_INIT = \"a photo of a\"  # initialization words\n",
    "    cfg.TRAINER.PROMETAR.PREC = \"fp16\"  # fp16, fp32, amp\n",
    "    cfg.TRAINER.PROMETAR.PROMPT_DEPTH_VISION = 9  # Max 12, minimum 0, for 0 it will be using shallow IVLP prompting (J=1)\n",
    "    cfg.TRAINER.PROMETAR.PROMPT_DEPTH_TEXT = 9  # Max 12, minimum 0, for 0 it will be using shallow IVLP prompting (J=1)\n",
    "    cfg.DATASET.SUBSAMPLE_CLASSES = \"all\"  # all, base or new\n",
    "    cfg.TRAINER.PROMETAR.ADAPT_LR = 0.0005\n",
    "    cfg.TRAINER.PROMETAR.LR_RATIO = 0.0005\n",
    "    cfg.TRAINER.PROMETAR.FAST_ADAPTATION = False\n",
    "    cfg.TRAINER.PROMETAR.MIXUP_ALPHA = 0.5\n",
    "    cfg.TRAINER.PROMETAR.MIXUP_BETA = 0.5\n",
    "    cfg.TRAINER.PROMETAR.DIM_RATE=8\n",
    "    cfg.OPTIM_VNET = CN()\n",
    "    cfg.OPTIM_VNET.NAME = \"adam\"\n",
    "    cfg.OPTIM_VNET.LR = 0.0003\n",
    "    cfg.OPTIM_VNET.WEIGHT_DECAY = 5e-4\n",
    "    cfg.OPTIM_VNET.MOMENTUM = 0.9\n",
    "    cfg.OPTIM_VNET.SGD_DAMPNING = 0\n",
    "    cfg.OPTIM_VNET.SGD_NESTEROV = False\n",
    "    cfg.OPTIM_VNET.RMSPROP_ALPHA = 0.99\n",
    "    cfg.OPTIM_VNET.ADAM_BETA1 = 0.9\n",
    "    cfg.OPTIM_VNET.ADAM_BETA2 = 0.999\n",
    "    cfg.OPTIM_VNET.STAGED_LR = False\n",
    "    cfg.OPTIM_VNET.NEW_LAYERS = ()\n",
    "    cfg.OPTIM_VNET.BASE_LR_MULT = 0.1\n",
    "    # Learning rate scheduler\n",
    "    cfg.OPTIM_VNET.LR_SCHEDULER = \"single_step\"\n",
    "    # -1 or 0 means the stepsize is equal to max_epoch\n",
    "    cfg.OPTIM_VNET.STEPSIZE = (-1, )\n",
    "    cfg.OPTIM_VNET.GAMMA = 0.1\n",
    "    cfg.OPTIM_VNET.MAX_EPOCH = 10\n",
    "    # Set WARMUP_EPOCH larger than 0 to activate warmup training\n",
    "    cfg.OPTIM_VNET.WARMUP_EPOCH = -1\n",
    "    # Either linear or constant\n",
    "    cfg.OPTIM_VNET.WARMUP_TYPE = \"linear\"\n",
    "    # Constant learning rate when type=constant\n",
    "    cfg.OPTIM_VNET.WARMUP_CONS_LR = 1e-5\n",
    "    # Minimum learning rate when type=linear\n",
    "    cfg.OPTIM_VNET.WARMUP_MIN_LR = 1e-5\n",
    "    # Recount epoch for the next scheduler (last_epoch=-1)\n",
    "    # Otherwise last_epoch=warmup_epoch\n",
    "    cfg.OPTIM_VNET.WARMUP_RECOUNT = True\n",
    "\n",
    "def setup_cfg(args):\n",
    "    cfg = get_cfg_default()\n",
    "    extend_cfg(cfg)\n",
    "    # 1. From the dataset config file\n",
    "    if args.dataset_config_file:\n",
    "        cfg.merge_from_file(args.dataset_config_file)\n",
    "    # 2. From the method config file\n",
    "    if args.config_file:\n",
    "        cfg.merge_from_file(args.config_file)\n",
    "    # 3. From input arguments\n",
    "    reset_cfg(cfg, args)\n",
    "    cfg.freeze()\n",
    "    return cfg\n",
    "\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "def load_clip_to_cpu(cfg): # Load CLIP\n",
    "    backbone_name = cfg.MODEL.BACKBONE.NAME\n",
    "    url = clip._MODELS[backbone_name]\n",
    "    model_path = clip._download(url)\n",
    "\n",
    "    try:\n",
    "        # loading JIT archive\n",
    "        model = torch.jit.load(model_path, map_location=\"cpu\").eval()\n",
    "        state_dict = None\n",
    "\n",
    "    except RuntimeError:\n",
    "        state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "\n",
    "    if cfg.TRAINER.NAME == \"\":\n",
    "      design_trainer = \"CoOp\"\n",
    "    else:\n",
    "      design_trainer = cfg.TRAINER.NAME\n",
    "    design_details = {\"trainer\": design_trainer,\n",
    "                      \"vision_depth\": 0,\n",
    "                      \"language_depth\": 0, \"vision_ctx\": 0,\n",
    "                      \"language_ctx\": 0}\n",
    "    model = clip.build_model(state_dict or model.state_dict(), design_details)\n",
    "\n",
    "    return model\n",
    "\n",
    "from dassl.config import get_cfg_default\n",
    "cfg = get_cfg_default()\n",
    "cfg.MODEL.BACKBONE.NAME = \"ViT-B/16\" # Set the vision encoder backbone of CLIP to ViT.\n",
    "clip_model = load_clip_to_cpu(cfg)\n",
    "\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model): # 초기화 하는 함수\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts): # 모델 호출\n",
    "        x = prompts + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "@TRAINER_REGISTRY.register(force=True)\n",
    "class CoCoOp(TrainerX):\n",
    "    def check_cfg(self, cfg):\n",
    "        assert cfg.TRAINER.COCOOP.PREC in [\"fp16\", \"fp32\", \"amp\"]\n",
    "\n",
    "    def build_model(self):\n",
    "        cfg = self.cfg\n",
    "        classnames = self.dm.dataset.classnames\n",
    "        print(f\"Loading CLIP (backbone: {cfg.MODEL.BACKBONE.NAME})\")\n",
    "        clip_model = load_clip_to_cpu(cfg)\n",
    "\n",
    "        if cfg.TRAINER.COCOOP.PREC == \"fp32\" or cfg.TRAINER.COCOOP.PREC == \"amp\":\n",
    "            # CLIP's default precision is fp16\n",
    "            clip_model.float()\n",
    "\n",
    "        print(\"Building custom CLIP\")\n",
    "        self.model = CoCoOpCustomCLIP(cfg, classnames, clip_model)\n",
    "\n",
    "        print(\"Turning off gradients in both the image and the text encoder\")\n",
    "        name_to_update = \"prompt_learner\"\n",
    "\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name_to_update not in name:\n",
    "                param.requires_grad_(False)\n",
    "\n",
    "        # Double check\n",
    "        enabled = set()\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                enabled.add(name)\n",
    "        print(f\"Parameters to be updated: {enabled}\")\n",
    "\n",
    "        if cfg.MODEL.INIT_WEIGHTS:\n",
    "            load_pretrained_weights(self.model.prompt_learner, cfg.MODEL.INIT_WEIGHTS)\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        # NOTE: only give prompt_learner to the optimizer\n",
    "        self.optim = build_optimizer(self.model.prompt_learner, cfg.OPTIM)\n",
    "        self.sched = build_lr_scheduler(self.optim, cfg.OPTIM)\n",
    "        self.register_model(\"prompt_learner\", self.model.prompt_learner, self.optim, self.sched)\n",
    "\n",
    "        self.scaler = GradScaler() if cfg.TRAINER.COCOOP.PREC == \"amp\" else None\n",
    "\n",
    "        # Note that multi-gpu training could be slow because CLIP's size is\n",
    "        # big, which slows down the copy operation in DataParallel\n",
    "        device_count = torch.cuda.device_count()\n",
    "        if device_count > 1:\n",
    "            print(f\"Multiple GPUs detected (n_gpus={device_count}), use all of them!\")\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "\n",
    "    def before_train(self):\n",
    "        directory = self.cfg.OUTPUT_DIR\n",
    "        if self.cfg.RESUME:\n",
    "            directory = self.cfg.RESUME\n",
    "        self.start_epoch = self.resume_model_if_exist(directory)\n",
    "\n",
    "        # Remember the starting time (for computing the elapsed time)\n",
    "        self.time_start = time.time()\n",
    "\n",
    "\n",
    "    def forward_backward(self, batch):\n",
    "        image, label = self.parse_batch_train(batch)\n",
    "\n",
    "        model = self.model\n",
    "        optim = self.optim\n",
    "        scaler = self.scaler\n",
    "\n",
    "        prec = self.cfg.TRAINER.COCOOP.PREC\n",
    "        loss = model(image, label) # Input image 모델 통과\n",
    "        optim.zero_grad()\n",
    "        loss.backward() # Backward (역전파)\n",
    "        optim.step() # 모델 parameter update\n",
    "\n",
    "        loss_summary = {\"loss\": loss.item()}\n",
    "\n",
    "        if (self.batch_idx + 1) == self.num_batches:\n",
    "            self.update_lr()\n",
    "\n",
    "        return loss_summary\n",
    "\n",
    "    def parse_batch_train(self, batch):\n",
    "        input = batch[\"img\"]\n",
    "        label = batch[\"label\"]\n",
    "        input = input.to(self.device)\n",
    "        label = label.to(self.device)\n",
    "        return input, label\n",
    "\n",
    "    def load_model(self, directory, epoch=None):\n",
    "        if not directory:\n",
    "            print(\"Note that load_model() is skipped as no pretrained model is given\")\n",
    "            return\n",
    "\n",
    "        names = self.get_model_names()\n",
    "\n",
    "        # By default, the best model is loaded\n",
    "        model_file = \"model-best.pth.tar\"\n",
    "\n",
    "        if epoch is not None:\n",
    "            model_file = \"model.pth.tar-\" + str(epoch)\n",
    "\n",
    "        for name in names:\n",
    "            model_path = osp.join(directory, name, model_file)\n",
    "\n",
    "            if not osp.exists(model_path):\n",
    "                raise FileNotFoundError('Model not found at \"{}\"'.format(model_path))\n",
    "\n",
    "            checkpoint = load_checkpoint(model_path)\n",
    "            state_dict = checkpoint[\"state_dict\"]\n",
    "            epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "            # Ignore fixed token vectors\n",
    "            if \"token_prefix\" in state_dict:\n",
    "                del state_dict[\"token_prefix\"]\n",
    "\n",
    "            if \"token_suffix\" in state_dict:\n",
    "                del state_dict[\"token_suffix\"]\n",
    "\n",
    "            print(\"Loading weights to {} \" 'from \"{}\" (epoch = {})'.format(name, model_path, epoch))\n",
    "            # set strict=False\n",
    "            self._models[name].load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    def after_train(self):\n",
    "      print(\"Finish training\")\n",
    "\n",
    "      do_test = not self.cfg.TEST.NO_TEST\n",
    "      if do_test:\n",
    "          if self.cfg.TEST.FINAL_MODEL == \"best_val\":\n",
    "              print(\"Deploy the model with the best val performance\")\n",
    "              self.load_model(self.output_dir)\n",
    "          else:\n",
    "              print(\"Deploy the last-epoch model\")\n",
    "          acc = self.test()\n",
    "\n",
    "      # Show elapsed time\n",
    "      elapsed = round(time.time() - self.time_start)\n",
    "      elapsed = str(datetime.timedelta(seconds=elapsed))\n",
    "      print(f\"Elapsed: {elapsed}\")\n",
    "\n",
    "      # Close writer\n",
    "      self.close_writer()\n",
    "      return acc\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Generic training loops.\"\"\"\n",
    "        self.before_train()\n",
    "        for self.epoch in range(self.start_epoch, self.max_epoch):\n",
    "            self.before_epoch()\n",
    "            self.run_epoch()\n",
    "            self.after_epoch()\n",
    "        acc = self.after_train()\n",
    "        return acc\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--root\", type=str, default=\"data/\", help=\"path to dataset\")\n",
    "parser.add_argument(\"--output-dir\", type=str, default=\"outputs/cocoop3\", help=\"output directory\")\n",
    "parser.add_argument(\n",
    "    \"--seed\", type=int, default=1, help=\"only positive value enables a fixed seed\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--config-file\", type=str, default=\"configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\", help=\"path to config file\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dataset-config-file\",\n",
    "    type=str,\n",
    "    default=\"configs/datasets/eurosat.yaml\",\n",
    "    help=\"path to config file for dataset setup\",\n",
    ")\n",
    "parser.add_argument(\"--trainer\", type=str, default=\"CoOp\", help=\"name of trainer\")\n",
    "parser.add_argument(\"--eval-only\", action=\"store_true\", help=\"evaluation only\")\n",
    "parser.add_argument(\n",
    "    \"--model-dir\",\n",
    "    type=str,\n",
    "    default=\"\",\n",
    "    help=\"load model from this directory for eval-only mode\",\n",
    ")\n",
    "parser.add_argument(\"--train-batch-size\", type=int, default=4)\n",
    "parser.add_argument(\"--epoch\", type=int, default=10)\n",
    "parser.add_argument(\"--subsample-classes\", type=str, default=\"base\")\n",
    "parser.add_argument(\n",
    "    \"--load-epoch\", type=int, default=0, help=\"load model weights at this epoch for evaluation\"\n",
    ")\n",
    "args = parser.parse_args([])\n",
    "\n",
    "def main(args):\n",
    "    cfg = setup_cfg(args)\n",
    "    if cfg.SEED >= 0:\n",
    "        set_random_seed(cfg.SEED)\n",
    "\n",
    "    if torch.cuda.is_available() and cfg.USE_CUDA:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    trainer = build_trainer(cfg)\n",
    "    if args.eval_only:\n",
    "        trainer.load_model(args.model_dir, epoch=args.load_epoch)\n",
    "        acc = trainer.test()\n",
    "        return acc\n",
    "\n",
    "    acc = trainer.train()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3n9blo4JO7m"
   },
   "source": [
    "### **Q1.  Understanding and implementing CoCoOp**\n",
    "* We have learned how to define CoOp in Lab Session 4.\n",
    "\n",
    "* The main difference between CoOp and CoCoOp is **meta network** to extract image tokens that is added to the text prompt.\n",
    "\n",
    "* Based on the CoOp code given in Lab Session 4, fill-in-the-blank exercise to test your understanding of critical parts of the CoCoOp.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SONlVIhPH_qF"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CoCoOpPromptLearner(nn.Module):\n",
    "    def __init__(self, cfg, classnames, clip_model):\n",
    "        super().__init__()\n",
    "        n_cls = len(classnames)\n",
    "        n_ctx = cfg.TRAINER.COCOOP.N_CTX\n",
    "        ctx_init = cfg.TRAINER.COCOOP.CTX_INIT\n",
    "        dtype = clip_model.dtype\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        vis_dim = clip_model.visual.output_dim\n",
    "        clip_imsize = clip_model.visual.input_resolution\n",
    "        cfg_imsize = cfg.INPUT.SIZE[0]\n",
    "        assert cfg_imsize == clip_imsize, f\"cfg_imsize ({cfg_imsize}) must equal to clip_imsize ({clip_imsize})\"\n",
    "\n",
    "        if ctx_init:\n",
    "            # use given words to initialize context vectors\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt).type(dtype)\n",
    "            ctx_vectors = embedding[0, 1: 1 + n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            # random initialization\n",
    "            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=dtype)\n",
    "            nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "\n",
    "        print(f'Initial context: \"{prompt_prefix}\"')\n",
    "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
    "\n",
    "        self.ctx = nn.Parameter(ctx_vectors)  # Wrap the initialized prompts above as parameters to make them trainable.\n",
    "\n",
    "        ### Tokenize ###\n",
    "        classnames = [name.replace(\"_\", \" \") for name in classnames]  # 예) \"Forest\"\n",
    "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames] # 예) \"A photo of Forest.\"\n",
    "\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]) # 예) [49406, 320, 1125, 539...]\n",
    "\n",
    "\n",
    "\n",
    "        #####################################\n",
    "        ####### Q1. Fill in the blank #######\n",
    "        ########## Define Meta Net ##########\n",
    "        self.meta_net = nn.Sequential(OrderedDict([\n",
    "            #(\"linear1\", \"fill in here\"(vis_dim, vis_dim // 16)),\n",
    "            (\"linear1\", nn.Linear(vis_dim, vis_dim // 16)),\n",
    "            (\"relu\", nn.ReLU(inplace=True)),\n",
    "            (\"linear2\", nn.Linear(vis_dim // 16, ctx_dim))\n",
    "        ]))\n",
    "        #####################################\n",
    "        ## Hint: meta network is composed to linear layer, relu activation, and linear layer.\n",
    "\n",
    "\n",
    "\n",
    "        if cfg.TRAINER.COCOOP.PREC == \"fp16\":\n",
    "            self.meta_net.half()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts).type(dtype)\n",
    "\n",
    "        # These token vectors will be saved when in save_model(),\n",
    "        # but they should be ignored in load_model() as we want to use\n",
    "        # those computed using the current class names\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx:, :])  # CLS, EOS\n",
    "        self.n_cls = n_cls\n",
    "        self.n_ctx = n_ctx\n",
    "        self.tokenized_prompts = tokenized_prompts  # torch.Tensor\n",
    "        self.name_lens = name_lens\n",
    "\n",
    "    def construct_prompts(self, ctx, prefix, suffix, label=None):\n",
    "        # dim0 is either batch_size (during training) or n_cls (during testing)\n",
    "        # ctx: context tokens, with shape of (dim0, n_ctx, ctx_dim)\n",
    "        # prefix: the sos token, with shape of (n_cls, 1, ctx_dim)\n",
    "        # suffix: remaining tokens, with shape of (n_cls, *, ctx_dim)\n",
    "\n",
    "        if label is not None:\n",
    "            prefix = prefix[label]\n",
    "            suffix = suffix[label]\n",
    "\n",
    "        prompts = torch.cat(\n",
    "            [\n",
    "                prefix,  # (dim0, 1, dim)\n",
    "                ctx,  # (dim0, n_ctx, dim)\n",
    "                suffix,  # (dim0, *, dim)\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        return prompts\n",
    "\n",
    "    def forward(self, im_features):\n",
    "        prefix = self.token_prefix\n",
    "        suffix = self.token_suffix\n",
    "        ctx = self.ctx  # (n_ctx, ctx_dim)\n",
    "\n",
    "\n",
    "\n",
    "        ############################################\n",
    "        ########## Q2,3. Fill in the blank #########\n",
    "        #bias = self.meta_net(\"Fill in here, Hint: Image feature is given as input to meta network\")  # (batch, ctx_dim)\n",
    "        bias = self.meta_net(im_features)  # (batch, ctx_dim)\n",
    "        bias = bias.unsqueeze(1)  # (batch, 1, ctx_dim)\n",
    "        ctx = ctx.unsqueeze(0)  # (1, n_ctx, ctx_dim)\n",
    "        #ctx_shifted = ctx + \"Fill in here, Hint: Add meta token to context token\"  # (batch, n_ctx, ctx_dim)\n",
    "        ctx_shifted = ctx + bias  # (batch, n_ctx, ctx_dim)\n",
    "        ############################################\n",
    "        ############################################\n",
    "\n",
    "\n",
    "\n",
    "        # Use instance-conditioned context tokens for all classes\n",
    "        prompts = []\n",
    "        for ctx_shifted_i in ctx_shifted:\n",
    "            ctx_i = ctx_shifted_i.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
    "            pts_i = self.construct_prompts(ctx_i, prefix, suffix)  # (n_cls, n_tkn, ctx_dim)\n",
    "            prompts.append(pts_i)\n",
    "        prompts = torch.stack(prompts)\n",
    "\n",
    "        return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "L_BluKEdKA94"
   },
   "outputs": [],
   "source": [
    "class CoCoOpCustomCLIP(nn.Module):\n",
    "    def __init__(self, cfg, classnames, clip_model):\n",
    "        super().__init__()\n",
    "        self.prompt_learner = CoCoOpPromptLearner(cfg, classnames, clip_model)\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.text_encoder = TextEncoder(clip_model)\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, image, label=None):\n",
    "        tokenized_prompts = self.tokenized_prompts\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "\n",
    "        image_features = self.image_encoder(image.type(self.dtype))\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "        ############################################\n",
    "        ########## Q4. Fill in the blank #########\n",
    "        #prompts = self.prompt_learner(\"Fill in here\")\n",
    "        prompts = self.prompt_learner(image_features)\n",
    "        ############################################\n",
    "        ############################################\n",
    "\n",
    "\n",
    "        logits = []\n",
    "        for pts_i, imf_i in zip(prompts, image_features):\n",
    "            text_features = self.text_encoder(pts_i, tokenized_prompts)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "            l_i = logit_scale * imf_i @ text_features.t()\n",
    "            logits.append(l_i)\n",
    "        logits = torch.stack(logits)\n",
    "\n",
    "        if self.prompt_learner.training:\n",
    "            return F.cross_entropy(logits, label)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CGZlqo-HtRN"
   },
   "source": [
    "### **Q2. Trainining CoCoOp**\n",
    "\n",
    "In this task, you will train CoCoOp on the EuroSAT dataset. If your implementation of CoCoOp in Question 1 is correct, the following code should execute without errors. Please submit the execution file so we can evaluate whether your code runs without any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XpakfLezALMy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/and___young/Documents/@24-2/DeepLearning/COSE474_NayoungKim/HW4/ProMetaR\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Zy3bAMnBMrXP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trainer: CoCoOp\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /Users/and___young/Documents/@24-2/DeepLearning/COSE474_NayoungKim/HW4/ProMetaR/data/eurosat/split_zhou_EuroSAT.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/and___young/Documents/@24-2/DeepLearning/COSE474_NayoungKim/HW4/ProMetaR/dassl/utils/tools.py:55: UserWarning: No file found at \"/Users/and___young/Documents/@24-2/DeepLearning/COSE474_NayoungKim/HW4/ProMetaR/data/eurosat/2750/AnnualCrop/AnnualCrop_1381.jpg\"\n",
      "  warnings.warn('No file found at \"{}\"'.format(fpath))\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m args\u001b[38;5;241m.\u001b[39msubsample_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m args\u001b[38;5;241m.\u001b[39meval_only \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m cocoop_base_acc \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 419\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mand\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mUSE_CUDA:\n\u001b[1;32m    417\u001b[0m     torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39mbenchmark \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39meval_only:\n\u001b[1;32m    421\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mload_model(args\u001b[38;5;241m.\u001b[39mmodel_dir, epoch\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mload_epoch)\n",
      "File \u001b[0;32m~/Documents/@24-2/DeepLearning/COSE474_NayoungKim/HW4/ProMetaR/dassl/engine/build.py:11\u001b[0m, in \u001b[0;36mbuild_trainer\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mVERBOSE:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading trainer: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(cfg\u001b[38;5;241m.\u001b[39mTRAINER\u001b[38;5;241m.\u001b[39mNAME))\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTRAINER_REGISTRY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAINER\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/@24-2/DeepLearning/COSE474_NayoungKim/HW4/ProMetaR/dassl/engine/trainer.py:324\u001b[0m, in \u001b[0;36mSimpleTrainer.__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dir \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mOUTPUT_DIR\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg \u001b[38;5;241m=\u001b[39m cfg\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_data_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_model()\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluator \u001b[38;5;241m=\u001b[39m build_evaluator(cfg, lab2cname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlab2cname)\n",
      "File \u001b[0;32m~/Documents/@24-2/DeepLearning/COSE474_NayoungKim/HW4/ProMetaR/dassl/engine/trainer.py:347\u001b[0m, in \u001b[0;36mSimpleTrainer.build_data_loader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_data_loader\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    342\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create essential data-related attributes.\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    A re-implementation of this method must create the\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03m    same attributes (self.dm is optional).\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m     dm \u001b[38;5;241m=\u001b[39m \u001b[43mDataManager\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader_x \u001b[38;5;241m=\u001b[39m dm\u001b[38;5;241m.\u001b[39mtrain_loader_x\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader_u \u001b[38;5;241m=\u001b[39m dm\u001b[38;5;241m.\u001b[39mtrain_loader_u  \u001b[38;5;66;03m# optional, can be None\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/@24-2/DeepLearning/COSE474_NayoungKim/HW4/ProMetaR/dassl/data/data_manager.py:61\u001b[0m, in \u001b[0;36mDataManager.__init__\u001b[0;34m(self, cfg, custom_tfm_train, custom_tfm_test, dataset_wrapper)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     55\u001b[0m     cfg,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m ):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# Build transform\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m custom_tfm_train \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/@24-2/DeepLearning/COSE474_NayoungKim/HW4/ProMetaR/dassl/data/datasets/build.py:11\u001b[0m, in \u001b[0;36mbuild_dataset\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mVERBOSE:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading dataset: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(cfg\u001b[38;5;241m.\u001b[39mDATASET\u001b[38;5;241m.\u001b[39mNAME))\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDATASET_REGISTRY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDATASET\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/@24-2/DeepLearning/COSE474_NayoungKim/HW4/ProMetaR/datasets/eurosat.py:38\u001b[0m, in \u001b[0;36mEuroSAT.__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m     35\u001b[0m mkdir_if_missing(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_fewshot_dir)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_path):\n\u001b[0;32m---> 38\u001b[0m     train, val, test \u001b[38;5;241m=\u001b[39m \u001b[43mOxfordPets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_split\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     train, val, test \u001b[38;5;241m=\u001b[39m DTD\u001b[38;5;241m.\u001b[39mread_and_split_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_dir, new_cnames\u001b[38;5;241m=\u001b[39mNEW_CNAMES)\n",
      "File \u001b[0;32m~/Documents/@24-2/DeepLearning/COSE474_NayoungKim/HW4/ProMetaR/datasets/oxford_pets.py:134\u001b[0m, in \u001b[0;36mOxfordPets.read_split\u001b[0;34m(filepath, path_prefix)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading split from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    133\u001b[0m split \u001b[38;5;241m=\u001b[39m read_json(filepath)\n\u001b[0;32m--> 134\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m val \u001b[38;5;241m=\u001b[39m _convert(split[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    136\u001b[0m test \u001b[38;5;241m=\u001b[39m _convert(split[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/@24-2/DeepLearning/COSE474_NayoungKim/HW4/ProMetaR/datasets/oxford_pets.py:128\u001b[0m, in \u001b[0;36mOxfordPets.read_split.<locals>._convert\u001b[0;34m(items)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m impath, label, classname \u001b[38;5;129;01min\u001b[39;00m items:\n\u001b[1;32m    127\u001b[0m     impath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path_prefix, impath)\n\u001b[0;32m--> 128\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[43mDatum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m     out\u001b[38;5;241m.\u001b[39mappend(item)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Documents/@24-2/DeepLearning/COSE474_NayoungKim/HW4/ProMetaR/dassl/data/datasets/base_dataset.py:24\u001b[0m, in \u001b[0;36mDatum.__init__\u001b[0;34m(self, impath, label, domain, classname)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, impath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, domain\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, classname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(impath, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m check_isfile(impath)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impath \u001b[38;5;241m=\u001b[39m impath\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label \u001b[38;5;241m=\u001b[39m label\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train on the Base Classes Train split and evaluate accuracy on the Base Classes Test split.\n",
    "args.trainer = \"CoCoOp\"\n",
    "args.train_batch_size = 4\n",
    "args.epoch = 100\n",
    "args.output_dir = \"outputs/cocoop\"\n",
    "\n",
    "args.subsample_classes = \"base\"\n",
    "args.eval_only = False\n",
    "cocoop_base_acc = main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xql7WpJ5vPII"
   },
   "outputs": [],
   "source": [
    "# Accuracy on the New Classes.\n",
    "args.model_dir = \"outputs/cocoop\"\n",
    "args.output_dir = \"outputs/cocoop/new_classes\"\n",
    "args.subsample_classes = \"new\"\n",
    "args.load_epoch = 100\n",
    "args.eval_only = True\n",
    "coop_novel_acc = main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1KdgiKFsowj"
   },
   "source": [
    "### **Q3. Analyzing the results of CoCoOp**\n",
    "Compare the results of CoCoOp with those of CoOp that we trained in Lab Session 4. Discuss possible reasons for the performance differences observed between CoCoOp and CoOp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJOBsn9HOOe1"
   },
   "source": [
    "# Results of CoOp (Lab Session 4)\n",
    "### - Train on the Base Classes Train split and evaluate accuracy on the Base Classes Test split.\n",
    "100%|██████████| 42/42 [00:19<00:00,  2.17it/s]=> result\n",
    "* total: 4,200\n",
    "* correct: 3,839\n",
    "* accuracy: 91.4%\n",
    "* error: 8.6%\n",
    "* macro_f1: 91.5%\n",
    "Elapsed: 0:02:53\n",
    "\n",
    "### - Accuracy on the New Classes.\n",
    "Evaluate on the *test* set\n",
    "100%|██████████| 39/39 [00:18<00:00,  2.11it/s]=> result\n",
    "* total: 3,900\n",
    "* correct: 2,007\n",
    "* accuracy: 51.5%\n",
    "* error: 48.5%\n",
    "* macro_f1: 45.6%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqPs6MSzOOKe"
   },
   "source": [
    "# Results of CoCoOp\n",
    "### - Train on the Base Classes Train split and evaluate accuracy on the Base Classes Test split.\n",
    "Evaluate on the *test* set\n",
    "100%|██████████| 42/42 [01:13<00:00,  1.74s/it]=> result\n",
    "* total: 4,200\n",
    "* correct: 3,813\n",
    "* accuracy: 90.8%\n",
    "* error: 9.2%\n",
    "* macro_f1: 90.9%\n",
    "Elapsed: 0:06:47\n",
    "\n",
    "### - Accuracy on the New Classes.\n",
    "Evaluate on the *test* set\n",
    "100%|██████████| 39/39 [01:06<00:00,  1.71s/it]=> result\n",
    "* total: 3,900\n",
    "* correct: 1,687\n",
    "* accuracy: 43.3%\n",
    "* error: 56.7%\n",
    "* macro_f1: 39.0%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkQXoT7EOPHu"
   },
   "source": [
    "# Comparison\n",
    "### For Base Classes:\n",
    "The performance is fairly similar, with CoOp performing slightly better (0.6% higher accuracy). This suggests that both methods are effective at learning the base classes during training. The slightly lower performance of CoCoOp might be due to the additional complexity of the conditional prompts, which could make optimization somewhat harder during training.\n",
    "\n",
    "### For New Classes:\n",
    "There's a considerable difference in performance, with CoOp outperforming CoCoOp by 8.2% in accuracy. This is somewhat surprising since CoCoOp was designed to improve generalization to new classes.\n",
    "I tried to come up with some possible reasons for this unexpected result, and those are as follows.\n",
    "\n",
    "a. Hyperparameter sensitivity: CoCoOp might be more sensitive to hyperparameter choices and could require more careful tuning.\n",
    "\n",
    "b. Dataset characteristics: The specific characteristics of the dataset might favor CoOp's simpler approach.\n",
    "\n",
    "c. Training stability: The meta-learning aspect of CoCoOp might make training less stable, potentially leading to suboptimal solutions.\n",
    "\n",
    "d. Context length: If the context length used for CoCoOp was not optimal, it could impact its generalization ability.\n",
    "\n",
    "\n",
    "### In perspective of Training Time:\n",
    "CoCoOp takes much longer to train (6:47 vs 2:53 for base classes) in this homework implementation.\n",
    "This increased computational cost doesn't translate to better performance in the implementation. For further studies I think experiment with different hyperparameters for CoCoOp, especially the context length and learning rate will be needed. A detailed analysis of the training process and hyperparameter choices may help identify the cause of this result."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
