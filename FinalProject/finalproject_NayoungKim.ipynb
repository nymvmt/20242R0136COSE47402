{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions - Report\n",
    "--------------\n",
    "motivations\n",
    "related works\n",
    "methods (formulation, architecture)\n",
    "- Describe your computing resources and reformulate your problem managable: Colab Pro $10/month \n",
    "experiments (data preparation, hyperparameter tuning, quantitative/qualitative experimental results)\n",
    "discussion & future direction\n",
    "--------------\n",
    "## basic points (overall) (2): \n",
    "- length (0.5)\n",
    "- format (0.5)\n",
    "- clarity of writing (1)\n",
    "\n",
    "## Introduction (5): \n",
    "- motivation (2)\n",
    "- problem definition (2)\n",
    "- concise description of contribution (1)\n",
    "\n",
    "## Methods (5): \n",
    "- significance/novelty (2)\n",
    "- figure (1)\n",
    "- reproducibility (2)-algorithm\n",
    ": 수도코드 등 implementation의 architecture를 설계하는 파트가 포함이 되어야 한다.\n",
    "\n",
    "## Experiments (7): \n",
    "- dataset (1)\n",
    "- computer resource (CPU,GPU, OS, pytorch etc.) & experimental design (1)\n",
    "- quantitative results (1)\n",
    "  : -> 숫자, Plot(그래프 그림) 얘는 정량적 결과(quantitative)\n",
    "- qualitative results (1)\n",
    "  : -> 수로는 설명되지 않는, 그림 같은 느낌적인 느낌을 전달하는. 정성적.(ex 어텐션 맵)\n",
    "- Figures (plots)/Tables and their analysis (2)\n",
    "  : Visualising result를 통해 result가 좋은지 아닌지를 확인할 것.\n",
    "- discussion why the proposed method is successful or unsuccessful (1) \n",
    "  : If your model is not competitive(degradation이 observe된다면), why&future direction 설명하면 됨.\n",
    "\n",
    "## Future direction (1).\n",
    "\n",
    "## Github history (2)\n",
    "\n",
    "## Overleaf history (2)\n",
    "## (Bonus+1) \n",
    "- pre-trained foundation models beyond ImageNet-pretrained CNNs (distillation, adaption, pseudo-labeling, baseline etc.), CLIP, BERT, RoBERTa\n",
    "- CLIP처럼 foundation model을 선택해서 했으면 좋겠다… stable diffusion… se(segment efficient)m2,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP model 기본 구조 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from clip import clip\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "import os\n",
    "\n",
    "# 토크나이저 초기화 \n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        # CLIP의 텍스트 인코더 컴포넌트를 가져온다\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts):\n",
    "        x = prompts + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
    "        return x\n",
    "\n",
    "def load_clip_to_cpu(model_name=\"ViT-B/16\"):\n",
    "    # 모델을 저장할 디렉토리 설정\n",
    "    root = os.path.expanduser(\"~/.cache/clip\")\n",
    "    \n",
    "    # CLIP 모델을 직접 로드한다\n",
    "    model, preprocess = clip.load(model_name, device=\"cpu\", download_root=root)\n",
    "    \n",
    "    # 평가 모드로 설정\n",
    "    model = model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def initialize_clip():\n",
    "    # CLIP 모델 로드\n",
    "    clip_model = load_clip_to_cpu()\n",
    "    \n",
    "    # 텍스트 인코더와 이미지 인코더 초기화\n",
    "    text_encoder = TextEncoder(clip_model)\n",
    "    image_encoder = clip_model.visual\n",
    "    \n",
    "    # GPU 사용 가능하면 GPU로 이동\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    text_encoder = text_encoder.to(device)\n",
    "    image_encoder = image_encoder.to(device)\n",
    "    \n",
    "    return text_encoder, image_encoder, device\n",
    "\n",
    "# 기본적인 데이터 전처리를 위한 transform 정의\n",
    "def get_transforms():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # CLIP 입력 크기에 맞춤\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                           (0.26862954, 0.26130258, 0.27577711))  # CLIP 기본 정규화 값\n",
    "    ])\n",
    "\n",
    "# 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    # CLIP 모델 초기화\n",
    "    text_encoder, image_encoder, device = initialize_clip()\n",
    "    print(f\"Device: {device}\")\n",
    "    print(\"CLIP model initialized successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
