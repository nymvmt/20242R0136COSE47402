{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "086b49ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import requests\n",
    "from torchvision.datasets import CIFAR100  # CIFAR-100 데이터셋 사용\n",
    "\n",
    "# 기본 설정\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "# CLIP 모델 로드 함수\n",
    "def load_clip_model(model_name=\"ViT-B/16\"):\n",
    "    import clip\n",
    "    model, preprocess = clip.load(model_name)\n",
    "    return model, preprocess\n",
    "\n",
    "# 데이터셋 클래스\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.dataset = CIFAR100(root=root, train=True, download=True)\n",
    "        self.transform = transform\n",
    "        self.classes = self.dataset.classes\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# 프롬프트 생성 함수\n",
    "def generate_prompts(classnames):\n",
    "    templates = [\n",
    "        'a photo of a {}.',\n",
    "        'an image of a {}.',\n",
    "        'this is a photo of a {}.',\n",
    "        'this is an image of a {}.',\n",
    "    ]\n",
    "    prompts = []\n",
    "    for template in templates:\n",
    "        prompts.extend([template.format(c) for c in classnames])\n",
    "    return prompts\n",
    "\n",
    "# 데이터 로더 설정\n",
    "def get_data_loaders(root='./data'):\n",
    "    # CLIP 기본 전처리\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), \n",
    "                           (0.26862954, 0.26130258, 0.27577711))\n",
    "    ])\n",
    "    \n",
    "    dataset = CustomDataset(root=root, transform=transform)\n",
    "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, \n",
    "                            shuffle=True, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    return train_loader, dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09c4fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CoOp(nn.Module):\n",
    "    def __init__(self, clip_model, classnames, n_ctx=16, ctx_init=\"a photo of a\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CLIP 모델의 텍스트 인코더\n",
    "        self.clip_model = clip_model\n",
    "        self.dtype = clip_model.dtype\n",
    "        \n",
    "        # Context length\n",
    "        n_cls = len(classnames)\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        \n",
    "        # 프롬프트 초기화\n",
    "        ctx_vectors = self.initialize_context_vectors(ctx_init, n_ctx, ctx_dim)\n",
    "        self.ctx = nn.Parameter(ctx_vectors)\n",
    "        \n",
    "        # 클래스명 토큰화\n",
    "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
    "        name_lens = [len(clip.tokenize(name)) for name in classnames]\n",
    "        prompts = [f\"{name}.\" for name in classnames]\n",
    "        \n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts])\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts).type(self.dtype)\n",
    "        \n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx:, :])  # CLS, EOS\n",
    "        \n",
    "        self.n_cls = n_cls\n",
    "        self.n_ctx = n_ctx\n",
    "        self.tokenized_prompts = tokenized_prompts\n",
    "    \n",
    "    def initialize_context_vectors(self, ctx_init, n_ctx, ctx_dim):\n",
    "        ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=self.dtype)\n",
    "        nn.init.normal_(ctx_vectors, std=0.02)\n",
    "        return ctx_vectors\n",
    "    \n",
    "    def forward(self, image):\n",
    "        # 이미지 인코딩\n",
    "        image_features = self.clip_model.encode_image(image)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # 프롬프트 구성\n",
    "        ctx = self.ctx\n",
    "        prefix = self.token_prefix\n",
    "        suffix = self.token_suffix\n",
    "        \n",
    "        # 프롬프트 임베딩 생성\n",
    "        prompts = torch.cat(\n",
    "            [\n",
    "                prefix,  # (n_cls, 1, dim)\n",
    "                ctx.unsqueeze(0).expand(self.n_cls, -1, -1),  # (n_cls, n_ctx, dim)\n",
    "                suffix,  # (n_cls, *, dim)\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        \n",
    "        # 텍스트 특징 추출\n",
    "        text_features = self.clip_model.encode_text(prompts)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # 유사도 계산\n",
    "        logit_scale = self.clip_model.logit_scale.exp()\n",
    "        logits = logit_scale * image_features @ text_features.t()\n",
    "        \n",
    "        return logits\n",
    "\n",
    "def train_coop(model, train_loader, num_epochs=50, device='cuda'):\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    best_acc = 0\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(images)\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 통계\n",
    "            total_loss += loss.item()\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            pbar.set_postfix({'loss': total_loss / (pbar.n + 1),\n",
    "                            'acc': 100 * correct / total})\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # 현재 정확도\n",
    "        epoch_acc = 100 * correct / total\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            torch.save(model.state_dict(), 'best_coop.pth')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Loss = {total_loss/len(train_loader):.4f}, '\n",
    "              f'Accuracy = {epoch_acc:.2f}%, Best = {best_acc:.2f}%')\n",
    "\n",
    "# 평가 함수\n",
    "def evaluate_coop(model, test_loader, device='cuda'):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc='Evaluating'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = model(images)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6009f3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d942eb76edb74d768ce6f28ff6230e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/169001437 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m clip_model, preprocess \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mViT-B/16\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 데이터 로더 준비\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m train_loader, test_loader, classnames \u001b[38;5;241m=\u001b[39m get_data_loaders()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# CoOp 모델 초기화\u001b[39;00m\n\u001b[1;32m     11\u001b[0m coop_model \u001b[38;5;241m=\u001b[39m CoOp(clip_model, classnames, n_ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "# CLIP 모델 로드\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "\n",
    "# 데이터 로더 준비\n",
    "train_loader, test_loader, classnames = get_data_loaders()\n",
    "\n",
    "# CoOp 모델 초기화\n",
    "coop_model = CoOp(clip_model, classnames, n_ctx=16)\n",
    "\n",
    "# 학습 실행\n",
    "train_coop(coop_model, train_loader, num_epochs=50, device=device)\n",
    "\n",
    "# 평가\n",
    "test_accuracy = evaluate_coop(coop_model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818a28dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
