{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation guide\n",
    "\n",
    "## Installation\n",
    "## Data Preparation\n",
    "## Foundation model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report guide\n",
    "--------------\n",
    "motivations\n",
    "related works\n",
    "methods (formulation, architecture)\n",
    "- Describe your computing resources and reformulate your problem managable: Colab Pro $10/month \n",
    "experiments (data preparation, hyperparameter tuning, quantitative/qualitative experimental results)\n",
    "discussion & future direction\n",
    "--------------\n",
    "## basic points (overall) (2): \n",
    "- length (0.5)\n",
    "- format (0.5)\n",
    "- clarity of writing (1)\n",
    "\n",
    "## Introduction (5): \n",
    "- motivation (2)\n",
    "- problem definition (2)\n",
    "- concise description of contribution (1)\n",
    "\n",
    "## Methods (5): \n",
    "- significance/novelty (2)\n",
    "- figure (1)\n",
    "- reproducibility (2)-algorithm\n",
    ": 수도코드 등 implementation의 architecture를 설계하는 파트가 포함이 되어야 한다.\n",
    "\n",
    "## Experiments (7): \n",
    "- dataset (1)\n",
    "- computer resource (CPU,GPU, OS, pytorch etc.) & experimental design (1)\n",
    "- quantitative results (1)\n",
    "  : -> 숫자, Plot(그래프 그림) 얘는 정량적 결과(quantitative)\n",
    "- qualitative results (1)\n",
    "  : -> 수로는 설명되지 않는, 그림 같은 느낌적인 느낌을 전달하는. 정성적.(ex 어텐션 맵)\n",
    "- Figures (plots)/Tables and their analysis (2)\n",
    "  : Visualising result를 통해 result가 좋은지 아닌지를 확인할 것.\n",
    "- discussion why the proposed method is successful or unsuccessful (1) \n",
    "  : If your model is not competitive(degradation이 observe된다면), why&future direction 설명하면 됨.\n",
    "\n",
    "## Future direction (1).\n",
    "\n",
    "## Github history (2)\n",
    "\n",
    "## Overleaf history (2)\n",
    "## (Bonus+1) \n",
    "- pre-trained foundation models beyond ImageNet-pretrained CNNs (distillation, adaption, pseudo-labeling, baseline etc.), CLIP, BERT, RoBERTa\n",
    "- CLIP처럼 foundation model을 선택해서 했으면 좋겠다… stable diffusion… se(segment efficient)m2,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP model 기본 구조 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/4p/kqk9nd_51cd1t2bqc3l1fyjr0000gn/T/pip-req-build-2tgydonu\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/4p/kqk9nd_51cd1t2bqc3l1fyjr0000gn/T/pip-req-build-2tgydonu\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from clip==1.0) (6.1.1)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from clip==1.0) (23.1)\n",
      "Requirement already satisfied: regex in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from clip==1.0) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from clip==1.0) (4.66.1)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from clip==1.0) (1.11.0)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from clip==1.0) (0.12.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from ftfy->clip==1.0) (0.2.6)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torch->clip==1.0) (4.6.3)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torchvision->clip==1.0) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torchvision->clip==1.0) (10.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->torchvision->clip==1.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->torchvision->clip==1.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->torchvision->clip==1.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->torchvision->clip==1.0) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "CLIP model initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CLIP의 기본 구성 요소만 초기화\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from clip import clip\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "import os\n",
    "\n",
    "# 토크나이저 초기화 \n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        # CLIP의 텍스트 인코더 컴포넌트를 가져온다\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts):\n",
    "        x = prompts + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
    "        return x\n",
    "\n",
    "def load_clip_to_cpu(model_name=\"ViT-B/16\"):\n",
    "    # 모델을 저장할 디렉토리 설정\n",
    "    root = os.path.expanduser(\"~/.cache/clip\")\n",
    "    \n",
    "    # CLIP 모델을 직접 로드한다\n",
    "    model, preprocess = clip.load(model_name, device=\"cpu\", download_root=root)\n",
    "    \n",
    "    # 평가 모드로 설정\n",
    "    model = model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def initialize_clip():\n",
    "    # CLIP 모델 로드\n",
    "    clip_model = load_clip_to_cpu()\n",
    "    \n",
    "    # 텍스트 인코더와 이미지 인코더 초기화\n",
    "    text_encoder = TextEncoder(clip_model)\n",
    "    image_encoder = clip_model.visual\n",
    "    \n",
    "    # GPU 사용 가능하면 GPU로 이동\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    text_encoder = text_encoder.to(device)\n",
    "    image_encoder = image_encoder.to(device)\n",
    "    \n",
    "    return text_encoder, image_encoder, device\n",
    "\n",
    "# 기본적인 데이터 전처리를 위한 transform 정의\n",
    "def get_transforms():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # CLIP 입력 크기에 맞춤\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                           (0.26862954, 0.26130258, 0.27577711))  # CLIP 기본 정규화 값\n",
    "    ])\n",
    "\n",
    "# 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    # CLIP 모델 초기화\n",
    "    text_encoder, image_encoder, device = initialize_clip()\n",
    "    print(f\"Device: {device}\")\n",
    "    print(\"CLIP model initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지-텍스트 쌍의 처리\n",
    "\n",
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.text_encoder = TextEncoder(clip_model)\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        # 이미지 인코딩\n",
    "        image_features = self.image_encoder(image.type(self.dtype))\n",
    "        image_features = F.normalize(image_features, dim=-1)\n",
    "\n",
    "        # 텍스트 인코딩\n",
    "        text_features = self.text_encoder(text)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "\n",
    "        # 유사도 계산\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits = logit_scale * image_features @ text_features.T\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수 정의\n",
    "\n",
    "def clip_loss(similarity):\n",
    "    labels = torch.arange(similarity.shape[0], device=similarity.device)\n",
    "    loss_i = F.cross_entropy(similarity, labels)\n",
    "    loss_t = F.cross_entropy(similarity.T, labels)\n",
    "    return (loss_i + loss_t) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 기능\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_image_features(model, image):\n",
    "    image_features = model.image_encoder(image.type(model.dtype))\n",
    "    return F.normalize(image_features, dim=-1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_text_features(model, text):\n",
    "    text_features = model.text_encoder(text)\n",
    "    return F.normalize(text_features, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 및 전처리\n",
    "\n",
    "def prepare_text(text):\n",
    "    return clip.tokenize(text).to(device)\n",
    "\n",
    "def prepare_image(image):\n",
    "    return get_transforms()(image).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoOp 핵심 아이디어 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoOpTextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model, n_ctx=16, n_cls=1000):\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.dtype = clip_model.dtype\n",
    "        \n",
    "        # CoOp의 핵심: 학습 가능한 context tokens\n",
    "        self.n_ctx = n_ctx\n",
    "        self.ctx = nn.Parameter(torch.randn(n_ctx, clip_model.transformer.width))\n",
    "        \n",
    "        # class-specific한 prompts를 위한 embedding\n",
    "        self.meta_net = nn.Sequential(OrderedDict([\n",
    "            (\"linear1\", nn.Linear(clip_model.transformer.width, clip_model.transformer.width // 16)),\n",
    "            (\"relu\", nn.ReLU(inplace=True)),\n",
    "            (\"linear2\", nn.Linear(clip_model.transformer.width // 16, n_cls * n_ctx * clip_model.transformer.width))\n",
    "        ]))\n",
    "        \n",
    "    def construct_prompts(self, ctx, prefix, suffix, label=None):\n",
    "        # ctx: context tokens\n",
    "        # prefix: \"A photo of a\"와 같은 고정된 프롬프트의 시작 부분\n",
    "        # suffix: 클래스 이름과 같은 프롬프트의 끝 부분\n",
    "        \n",
    "        if label is not None:\n",
    "            # class-specific prompts 생성\n",
    "            ctx = self.meta_net(ctx)\n",
    "            ctx = ctx.view(-1, self.n_ctx, self.transformer.width)\n",
    "            ctx = ctx[torch.arange(ctx.shape[0]), label]\n",
    "        \n",
    "        # 전체 프롬프트 구성\n",
    "        prompts = torch.cat(\n",
    "            [\n",
    "                prefix,  # (n_cls, prefix_len, dim)\n",
    "                ctx,     # (n_cls, n_ctx, dim)\n",
    "                suffix,  # (n_cls, suffix_len, dim)\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        return prompts\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts, label=None):\n",
    "        # 입력 임베딩 처리\n",
    "        x = prompts + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "        \n",
    "        # 토큰화된 프롬프트에 따라 특정 위치의 features 선택\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)]\n",
    "        \n",
    "        # 텍스트 프로젝션 적용\n",
    "        x = x @ self.text_projection\n",
    "        \n",
    "        return x\n",
    "\n",
    "def initialize_coop(clip_model, n_ctx=16, n_cls=1000):\n",
    "    # CoOp 텍스트 인코더와 이미지 인코더 초기화\n",
    "    text_encoder = CoOpTextEncoder(clip_model, n_ctx=n_ctx, n_cls=n_cls)\n",
    "    image_encoder = clip_model.visual\n",
    "    \n",
    "    # GPU 사용 가능하면 GPU로 이동\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    text_encoder = text_encoder.to(device)\n",
    "    image_encoder = image_encoder.to(device)\n",
    "    \n",
    "    return text_encoder, image_encoder, device\n",
    "\n",
    "# 학습을 위한 손실 함수\n",
    "def compute_loss(image_features, text_features, logit_scale):\n",
    "    # 정규화\n",
    "    image_features = F.normalize(image_features, dim=-1)\n",
    "    text_features = F.normalize(text_features, dim=-1)\n",
    "    \n",
    "    # 로짓 계산\n",
    "    logits = logit_scale.exp() * image_features @ text_features.t()\n",
    "    \n",
    "    # 레이블 생성 (대각선 매트릭스)\n",
    "    labels = torch.arange(logits.shape[0], device=logits.device)\n",
    "    \n",
    "    # 교차 엔트로피 손실 계산\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caltech-101 데이터셋을 사용한 CoOp (Context Optimization) 학습 및 시각화 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.6.17.tar.gz (82 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from kaggle) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from kaggle) (4.66.1)\n",
      "Collecting python-slugify (from kaggle)\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: urllib3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from kaggle) (1.26.15)\n",
      "Requirement already satisfied: bleach in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from kaggle) (6.0.0)\n",
      "Requirement already satisfied: webencodings in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from bleach->kaggle) (0.5.1)\n",
      "Collecting text-unidecode>=1.3 (from python-slugify->kaggle)\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->kaggle) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->kaggle) (3.4)\n",
      "Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.6.17-py3-none-any.whl size=105800 sha256=a1e6f8124d3fdb0b75c4f1b8edde6ee646f88fd0bdd3127b48ab89c8a758f571\n",
      "  Stored in directory: /Users/and___young/Library/Caches/pip/wheels/2b/af/a9/70bffa2773af622d2ebea9c8d407720b86e67bd40c465bf837\n",
      "Successfully built kaggle\n",
      "Installing collected packages: text-unidecode, python-slugify, kaggle\n",
      "Successfully installed kaggle-1.6.17 python-slugify-8.0.4 text-unidecode-1.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Could not find kaggle.json. Make sure it's located in /Users/and___young/.kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkaggle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkaggle_api_extended\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KaggleApi\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/kaggle/__init__.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkaggle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ApiClient\n\u001b[1;32m      6\u001b[0m api \u001b[38;5;241m=\u001b[39m KaggleApi(ApiClient())\n\u001b[0;32m----> 7\u001b[0m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthenticate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/kaggle/api/kaggle_api_extended.py:407\u001b[0m, in \u001b[0;36mKaggleApi.authenticate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 407\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not find \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Make sure it\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms located in\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    408\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Or use the environment method. See setup\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    409\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instructions at\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    410\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m https://github.com/Kaggle/kaggle-api/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    411\u001b[0m                           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_file, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_dir))\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# Step 3: load into configuration!\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_config(config_data)\n",
      "\u001b[0;31mOSError\u001b[0m: Could not find kaggle.json. Make sure it's located in /Users/and___young/.kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "def download_caltech101(root_dir='./data'):\n",
    "    # 디렉토리 생성\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"Downloading Caltech-101 dataset from Kaggle...\")\n",
    "    \n",
    "    # Kaggle API 초기화\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    \n",
    "    # 데이터셋 다운로드\n",
    "    api.dataset_download_files('huanghanchina/caltech101',\n",
    "                             path=root_dir,\n",
    "                             quiet=False)\n",
    "    \n",
    "    # 압축 해제\n",
    "    zip_path = os.path.join(root_dir, 'caltech101.zip')\n",
    "    print(\"\\nExtracting files...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(root_dir)\n",
    "    \n",
    "    # 파일 구조 정리\n",
    "    src_dir = os.path.join(root_dir, \"101_ObjectCategories\")\n",
    "    dst_dir = os.path.join(root_dir, \"caltech101\")\n",
    "    \n",
    "    if os.path.exists(dst_dir):\n",
    "        shutil.rmtree(dst_dir)\n",
    "    shutil.move(src_dir, dst_dir)\n",
    "    \n",
    "    # 임시 파일 삭제\n",
    "    os.remove(zip_path)\n",
    "    \n",
    "    print(f\"\\nDataset downloaded and extracted to {dst_dir}\")\n",
    "    print(\"Number of categories:\", len(os.listdir(dst_dir)))\n",
    "    return dst_dir\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 데이터셋 다운로드 및 설정\n",
    "    dataset_path = download_caltech101()\n",
    "    print(f\"Dataset ready at: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m Caltech101Dataset(root_dir\u001b[38;5;241m=\u001b[39m\u001b[43mdataset_path\u001b[49m, transform\u001b[38;5;241m=\u001b[39mtransform)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_path' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = Caltech101Dataset(root_dir=dataset_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Caltech-101 dataset...\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 500: INTERNAL SERVER ERROR",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 68\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dst_dir\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# 데이터셋 다운로드 및 설정\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     dataset_path \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_caltech101\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset ready at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m, in \u001b[0;36mdownload_caltech101\u001b[0;34m(root_dir)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 진행률 표시기와 함께 다운로드\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading Caltech-101 dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m     21\u001b[0m     total_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(response\u001b[38;5;241m.\u001b[39mheaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Length\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py:561\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    560\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py:641\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 500: INTERNAL SERVER ERROR"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "def download_caltech101(root_dir='./data'):\n",
    "    \"\"\"\n",
    "    Caltech-101 데이터셋을 다운로드하고 설정합니다.\n",
    "    \"\"\"\n",
    "    # 디렉토리 생성\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    \n",
    "    # 데이터셋 URL\n",
    "    url = \"https://data.caltech.edu/records/mzrjq-6wc02/files/caltech-101.tar.gz\"\n",
    "    filename = os.path.join(root_dir, \"caltech-101.tar.gz\")\n",
    "    \n",
    "    # 진행률 표시기와 함께 다운로드\n",
    "    print(\"Downloading Caltech-101 dataset...\")\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        total_size = int(response.headers['Content-Length'])\n",
    "        with open(filename, 'wb') as f:\n",
    "            with tqdm(total=total_size, unit='B', unit_scale=True) as pbar:\n",
    "                while True:\n",
    "                    chunk = response.read(8192)\n",
    "                    if not chunk:\n",
    "                        break\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "    \n",
    "    # 압축 해제\n",
    "    print(\"\\nExtracting files...\")\n",
    "    with tarfile.open(filename, 'r:gz') as tar:\n",
    "        def is_within_directory(directory, target):\n",
    "            abs_directory = os.path.abspath(directory)\n",
    "            abs_target = os.path.abspath(target)\n",
    "            prefix = os.path.commonprefix([abs_directory, abs_target])\n",
    "            return prefix == abs_directory\n",
    "        \n",
    "        def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):\n",
    "            for member in tar.getmembers():\n",
    "                member_path = os.path.join(path, member.name)\n",
    "                if not is_within_directory(path, member_path):\n",
    "                    raise Exception(\"Attempted path traversal in tar file\")\n",
    "            \n",
    "            tar.extractall(path, members, numeric_owner=numeric_owner) \n",
    "            \n",
    "        safe_extract(tar, path=root_dir)\n",
    "    \n",
    "    # 파일 구조 정리\n",
    "    src_dir = os.path.join(root_dir, \"caltech-101\", \"101_ObjectCategories\")\n",
    "    dst_dir = os.path.join(root_dir, \"caltech101\")\n",
    "    \n",
    "    if os.path.exists(dst_dir):\n",
    "        shutil.rmtree(dst_dir)\n",
    "    shutil.move(src_dir, dst_dir)\n",
    "    \n",
    "    # 임시 파일 삭제\n",
    "    os.remove(filename)\n",
    "    shutil.rmtree(os.path.join(root_dir, \"caltech-101\"))\n",
    "    \n",
    "    print(f\"\\nDataset downloaded and extracted to {dst_dir}\")\n",
    "    print(\"Number of categories:\", len(os.listdir(dst_dir)))\n",
    "    return dst_dir\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 데이터셋 다운로드 및 설정\n",
    "    dataset_path = download_caltech101()\n",
    "    print(f\"Dataset ready at: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/caltech101'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 173\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# 메인 실행\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# 학습 실행\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     train_losses, train_accuracies, test_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_coop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# 결과 시각화\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     visualize_training(train_losses, train_accuracies, test_accuracies)\n",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m, in \u001b[0;36mtrain_coop\u001b[0;34m(num_epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_coop\u001b[39m(num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# 데이터 로드\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     transform \u001b[38;5;241m=\u001b[39m get_transforms()\n\u001b[0;32m---> 33\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCaltech101Dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     test_dataset \u001b[38;5;241m=\u001b[39m Caltech101Dataset(train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m     36\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m, in \u001b[0;36mCaltech101Dataset.__init__\u001b[0;34m(self, root_dir, transform, train)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/caltech101\u001b[39m\u001b[38;5;124m'\u001b[39m, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mclasses\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# 학습/테스트 분할 (80:20)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/datasets/folder.py:310\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    304\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m ):\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/datasets/folder.py:145\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    137\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    143\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 145\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/datasets/folder.py:219\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    193\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/datasets/folder.py:41\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/caltech101'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "\n",
    "class Caltech101Dataset(Dataset):\n",
    "    def __init__(self, root_dir='./data/caltech101', transform=None, train=True):\n",
    "        self.dataset = datasets.ImageFolder(root_dir, transform=transform)\n",
    "        self.classes = self.dataset.classes\n",
    "        \n",
    "        # 학습/테스트 분할 (80:20)\n",
    "        train_size = int(0.8 * len(self.dataset))\n",
    "        test_size = len(self.dataset) - train_size\n",
    "        train_dataset, test_dataset = random_split(\n",
    "            self.dataset, [train_size, test_size],\n",
    "            generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "        \n",
    "        self.data = train_dataset if train else test_dataset\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.data[idx]\n",
    "        return image, label\n",
    "\n",
    "def train_coop(num_epochs=50, batch_size=32, learning_rate=1e-4):\n",
    "    # 데이터 로드\n",
    "    transform = get_transforms()\n",
    "    train_dataset = Caltech101Dataset(train=True, transform=transform)\n",
    "    test_dataset = Caltech101Dataset(train=False, transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # 모델 초기화\n",
    "    clip_model = load_clip_to_cpu()\n",
    "    text_encoder, image_encoder, device = initialize_coop(\n",
    "        clip_model, \n",
    "        n_ctx=16, \n",
    "        n_cls=len(train_dataset.classes)\n",
    "    )\n",
    "    \n",
    "    # 옵티마이저 설정\n",
    "    optimizer = torch.optim.Adam(text_encoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # 학습 기록용\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 학습 모드\n",
    "        text_encoder.train()\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # 이미지 features 추출\n",
    "            image_features = image_encoder(images)\n",
    "            \n",
    "            # 프롬프트 템플릿 생성\n",
    "            prefix = clip.tokenize(\"A photo of a\").to(device)\n",
    "            suffix = clip.tokenize([train_dataset.classes[label] for label in labels]).to(device)\n",
    "            \n",
    "            # 텍스트 프롬프트 생성 및 features 추출\n",
    "            prompts = text_encoder.construct_prompts(\n",
    "                text_encoder.ctx,\n",
    "                prefix=prefix,\n",
    "                suffix=suffix,\n",
    "                label=labels\n",
    "            )\n",
    "            text_features = text_encoder(prompts, suffix, labels)\n",
    "            \n",
    "            # 손실 계산\n",
    "            loss = compute_loss(image_features, text_features, clip_model.logit_scale)\n",
    "            \n",
    "            # 역전파\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 정확도 계산\n",
    "            logits = image_features @ text_features.t()\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # 테스트 평가\n",
    "        text_encoder.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                image_features = image_encoder(images)\n",
    "                prefix = clip.tokenize(\"A photo of a\").to(device)\n",
    "                suffix = clip.tokenize([test_dataset.classes[label] for label in labels]).to(device)\n",
    "                \n",
    "                prompts = text_encoder.construct_prompts(\n",
    "                    text_encoder.ctx,\n",
    "                    prefix=prefix,\n",
    "                    suffix=suffix,\n",
    "                    label=labels\n",
    "                )\n",
    "                text_features = text_encoder(prompts, suffix, labels)\n",
    "                \n",
    "                logits = image_features @ text_features.t()\n",
    "                pred = torch.argmax(logits, dim=1)\n",
    "                test_correct += (pred == labels).sum().item()\n",
    "                test_total += labels.size(0)\n",
    "        \n",
    "        # 에폭당 평균 손실과 정확도 기록\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        test_accuracy = 100 * test_correct / test_total\n",
    "        \n",
    "        train_losses.append(avg_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Training - Loss: {avg_loss:.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "        print(f\"Testing - Accuracy: {test_accuracy:.2f}%\\n\")\n",
    "    \n",
    "    return train_losses, train_accuracies, test_accuracies\n",
    "\n",
    "def visualize_training(train_losses, train_accuracies, test_accuracies):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # 손실 그래프\n",
    "    ax1.plot(train_losses)\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    \n",
    "    # 정확도 그래프\n",
    "    ax2.plot(train_accuracies, label='Train')\n",
    "    ax2.plot(test_accuracies, label='Test')\n",
    "    ax2.set_title('Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_learned_prompts(text_encoder, class_names):\n",
    "    ctx = text_encoder.ctx.detach().cpu()\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(ctx, cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.title('Learned Context Tokens')\n",
    "    plt.xlabel('Embedding Dimension')\n",
    "    plt.ylabel('Context Position')\n",
    "    plt.show()\n",
    "\n",
    "# 메인 실행\n",
    "if __name__ == \"__main__\":\n",
    "    # 학습 실행\n",
    "    train_losses, train_accuracies, test_accuracies = train_coop(\n",
    "        num_epochs=50,\n",
    "        batch_size=32,\n",
    "        learning_rate=1e-4\n",
    "    )\n",
    "    \n",
    "    # 결과 시각화\n",
    "    visualize_training(train_losses, train_accuracies, test_accuracies)\n",
    "    \n",
    "    # 학습된 프롬프트 시각화\n",
    "    clip_model = load_clip_to_cpu()\n",
    "    text_encoder, _, _ = initialize_coop(\n",
    "        clip_model,\n",
    "        n_ctx=16,\n",
    "        n_cls=101  # Caltech-101의 클래스 수\n",
    "    )\n",
    "    visualize_learned_prompts(text_encoder, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
