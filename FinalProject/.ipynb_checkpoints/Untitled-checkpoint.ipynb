{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4758433",
   "metadata": {},
   "source": [
    "# CLIP, CoOp, CoCoOp Implementation and Lightweight Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24540c1c",
   "metadata": {},
   "source": [
    "[0] 환경 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5b2b6c",
   "metadata": {},
   "source": [
    "# use dassl as a codebase to develop any deep learning projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92853c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/mlvlab/ProMetaR.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c243d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/4p/kqk9nd_51cd1t2bqc3l1fyjr0000gn/T/pip-req-build-ttoy108d\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/4p/kqk9nd_51cd1t2bqc3l1fyjr0000gn/T/pip-req-build-ttoy108d\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from clip==1.0) (6.1.1)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from clip==1.0) (23.1)\n",
      "Requirement already satisfied: regex in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from clip==1.0) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from clip==1.0) (4.66.1)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from clip==1.0) (1.11.0)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from clip==1.0) (0.12.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from ftfy->clip==1.0) (0.2.6)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torch->clip==1.0) (4.6.3)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torchvision->clip==1.0) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torchvision->clip==1.0) (10.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->torchvision->clip==1.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->torchvision->clip==1.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->torchvision->clip==1.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->torchvision->clip==1.0) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# CLIP 설치\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d560fe",
   "metadata": {},
   "source": [
    "[1] 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea1550d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "from clip import clip\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8586f7",
   "metadata": {},
   "source": [
    "[2] CLIP 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5edef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP 모델 다운로드 및 토크나이저 초기화\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "def load_clip_to_cpu(model_name=\"ViT-B/16\"):\n",
    "    model, _ = clip.load(model_name, device=\"cpu\")\n",
    "    return model.eval()\n",
    "\n",
    "# 기본 설정\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541d21e1",
   "metadata": {},
   "source": [
    "[3] 기본 TextEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74b9bea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts):\n",
    "        x = prompts + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5614813c",
   "metadata": {},
   "source": [
    "[4] CoOp 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11514bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoOpPromptLearner(nn.Module):\n",
    "    \"\"\"CoOp의 프롬프트 학습 모듈\"\"\"\n",
    "    def __init__(self, clip_model, n_ctx=16, n_cls=1000, ctx_init=\"a photo of a\"):\n",
    "        super().__init__()\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        dtype = clip_model.dtype\n",
    "        \n",
    "        # 프롬프트 초기화\n",
    "        if ctx_init:\n",
    "            # 주어진 단어로 초기화\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt).type(dtype)\n",
    "            ctx_vectors = embedding[0, 1: 1 + n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            # 랜덤 초기화\n",
    "            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=dtype)\n",
    "            nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "        \n",
    "        print(f'Initial context: \"{prompt_prefix}\"')\n",
    "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
    "        \n",
    "        # 컨텍스트 벡터를 학습 가능한 파라미터로 등록\n",
    "        self.ctx = nn.Parameter(ctx_vectors)\n",
    "        \n",
    "        # 클래스 토큰 처리\n",
    "        classnames = [name.replace(\"_\", \" \") for name in range(n_cls)]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
    "        \n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts])\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts).type(dtype)\n",
    "        \n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx:, :])\n",
    "        \n",
    "        self.n_cls = n_cls\n",
    "        self.n_ctx = n_ctx\n",
    "        self.tokenized_prompts = tokenized_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b4e17cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCLIP(nn.Module):\n",
    "    \"\"\"CoOp 전체 모델\"\"\"\n",
    "    def __init__(self, clip_model, prompt_learner):\n",
    "        super().__init__()\n",
    "        self.prompt_learner = prompt_learner\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.text_encoder = TextEncoder(clip_model)\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, image, label=None):\n",
    "        image_features = self.image_encoder(image.type(self.dtype))\n",
    "        text_features = self.text_encoder(self.prompts, self.tokenized_prompts)\n",
    "\n",
    "        # 정규화\n",
    "        image_features = F.normalize(image_features, dim=-1)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "\n",
    "        # 로짓 계산\n",
    "        logits = image_features @ text_features.t() * self.logit_scale.exp()\n",
    "\n",
    "        if label is not None:\n",
    "            loss = F.cross_entropy(logits, label)\n",
    "            return loss\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345c7e17",
   "metadata": {},
   "source": [
    "[5] CoCoOp 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8445d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoCoOpPromptLearner(nn.Module):\n",
    "    \"\"\"CoCoOp의 프롬프트 학습 모듈\"\"\"\n",
    "    def __init__(self, clip_model, n_ctx=4, n_cls=1000, ctx_init=\"a photo of a\"):\n",
    "        super().__init__()\n",
    "        self.n_cls = n_cls\n",
    "        self.n_ctx = n_ctx\n",
    "        \n",
    "        # CLIP 모델의 차원 정보\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        vis_dim = clip_model.visual.output_dim\n",
    "        clip_imsize = clip_model.visual.input_resolution\n",
    "        \n",
    "        # 프롬프트 초기화\n",
    "        if ctx_init:\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt).type(clip_model.dtype)\n",
    "            ctx_vectors = embedding[0, 1: 1 + n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=clip_model.dtype)\n",
    "            nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "        \n",
    "        print(f'Initial context: \"{prompt_prefix}\"')\n",
    "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
    "        \n",
    "        # 컨텍스트 벡터\n",
    "        self.ctx = nn.Parameter(ctx_vectors)\n",
    "        \n",
    "        # Meta Network\n",
    "        self.meta_net = nn.Sequential(OrderedDict([\n",
    "            (\"linear1\", nn.Linear(vis_dim, vis_dim // 16)),\n",
    "            (\"relu\", nn.ReLU(inplace=True)),\n",
    "            (\"linear2\", nn.Linear(vis_dim // 16, ctx_dim))\n",
    "        ]))\n",
    "        \n",
    "        # 클래스 토큰 처리\n",
    "        classnames = [name.replace(\"_\", \" \") for name in range(n_cls)]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
    "        \n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts])\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts).type(clip_model.dtype)\n",
    "        \n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx:, :])\n",
    "        \n",
    "        self.tokenized_prompts = tokenized_prompts\n",
    "\n",
    "    def forward(self, im_features):\n",
    "        prefix = self.token_prefix\n",
    "        suffix = self.token_suffix\n",
    "        ctx = self.ctx\n",
    "        \n",
    "        # Meta Network로 이미지 특징에서 컨텍스트 바이어스 생성\n",
    "        bias = self.meta_net(im_features)\n",
    "        bias = bias.unsqueeze(1)\n",
    "        ctx = ctx.unsqueeze(0)\n",
    "        ctx_shifted = ctx + bias\n",
    "        \n",
    "        # 프롬프트 생성\n",
    "        prompts = []\n",
    "        for ctx_shifted_i in ctx_shifted:\n",
    "            ctx_i = ctx_shifted_i.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
    "            pts_i = self.construct_prompts(ctx_i, prefix, suffix)\n",
    "            prompts.append(pts_i)\n",
    "        prompts = torch.stack(prompts)\n",
    "        \n",
    "        return prompts\n",
    "\n",
    "    def construct_prompts(self, ctx, prefix, suffix, label=None):\n",
    "        if label is not None:\n",
    "            prefix = prefix[label]\n",
    "            suffix = suffix[label]\n",
    "        \n",
    "        prompts = torch.cat([\n",
    "            prefix,\n",
    "            ctx,\n",
    "            suffix,\n",
    "        ], dim=1)\n",
    "        \n",
    "        return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d6ff9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoCoOpCustomCLIP(nn.Module):\n",
    "    \"\"\"CoCoOp 전체 모델\"\"\"\n",
    "    def __init__(self, clip_model, prompt_learner):\n",
    "        super().__init__()\n",
    "        self.prompt_learner = prompt_learner\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.text_encoder = TextEncoder(clip_model)\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, image, label=None):\n",
    "        image_features = self.image_encoder(image.type(self.dtype))\n",
    "        prompts = self.prompt_learner(image_features)\n",
    "        \n",
    "        logits = []\n",
    "        for pts_i, img_feat_i in zip(prompts, image_features):\n",
    "            text_features = self.text_encoder(pts_i, self.prompt_learner.tokenized_prompts)\n",
    "            logit = img_feat_i @ text_features.T\n",
    "            logits.append(logit)\n",
    "        logits = torch.stack(logits)\n",
    "        \n",
    "        if label is not None:\n",
    "            loss = F.cross_entropy(logits * self.logit_scale.exp(), label)\n",
    "            return loss\n",
    "        return logits * self.logit_scale.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fb01c0",
   "metadata": {},
   "source": [
    "[6] 경량화된 CoCoOp 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "307ef00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightweightCoCoOpPromptLearner(nn.Module):\n",
    "    \"\"\"경량화된 CoCoOp 프롬프트 학습 모듈\"\"\"\n",
    "    def __init__(self, clip_model, n_ctx=4, n_cls=1000, ctx_init=\"a photo of a\", reduction_factor=8):\n",
    "        super().__init__()\n",
    "        self.n_cls = n_cls\n",
    "        self.n_ctx = n_ctx\n",
    "        \n",
    "        # CLIP 모델의 차원 정보\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        vis_dim = clip_model.visual.output_dim\n",
    "        \n",
    "        # 축소된 차원 계산\n",
    "        self.reduced_ctx_dim = ctx_dim // reduction_factor\n",
    "        self.reduced_vis_dim = vis_dim // reduction_factor\n",
    "        \n",
    "        # 차원 축소/복원을 위한 레이어\n",
    "        self.ctx_reduction = nn.Linear(ctx_dim, self.reduced_ctx_dim)\n",
    "        self.ctx_expansion = nn.Linear(self.reduced_ctx_dim, ctx_dim)\n",
    "        \n",
    "        # 프롬프트 초기화 (축소된 차원으로)\n",
    "        if ctx_init:\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            prompt = clip.tokenize(ctx_init)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt).type(clip_model.dtype)\n",
    "                ctx_vectors = embedding[0, 1: 1 + n_ctx, :]\n",
    "                ctx_vectors = self.ctx_reduction(ctx_vectors)\n",
    "        else:\n",
    "            ctx_vectors = torch.empty(n_ctx, self.reduced_ctx_dim, dtype=clip_model.dtype)\n",
    "            nn.init.normal_(ctx_vectors, std=0.02)\n",
    "        \n",
    "        # 경량화된 컨텍스트 벡터\n",
    "        self.ctx = nn.Parameter(ctx_vectors)\n",
    "        \n",
    "        # 경량화된 Meta Network\n",
    "        self.meta_net = nn.Sequential(OrderedDict([\n",
    "            (\"reduction\", nn.Linear(vis_dim, self.reduced_vis_dim)),\n",
    "            (\"linear1\", nn.Linear(self.reduced_vis_dim, self.reduced_vis_dim // 4)),\n",
    "            (\"relu\", nn.ReLU(inplace=True)),\n",
    "            (\"linear2\", nn.Linear(self.reduced_vis_dim // 4, self.reduced_ctx_dim))\n",
    "        ]))\n",
    "        \n",
    "        # 토큰 처리 (기존과 동일)\n",
    "        classnames = [name.replace(\"_\", \" \") for name in range(n_cls)]\n",
    "        prompts = [ctx_init + \" \" + name + \".\" for name in classnames]\n",
    "        \n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts])\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts).type(clip_model.dtype)\n",
    "        \n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx:, :])\n",
    "        self.tokenized_prompts = tokenized_prompts\n",
    "\n",
    "    def forward(self, im_features):\n",
    "        # 이미지 특징 차원 축소 및 Meta Network 처리\n",
    "        reduced_features = self.meta_net(im_features)\n",
    "        bias = reduced_features.unsqueeze(1)\n",
    "        \n",
    "        # 컨텍스트 벡터 처리\n",
    "        ctx = self.ctx.unsqueeze(0)\n",
    "        ctx_shifted = ctx + bias\n",
    "        \n",
    "        # 차원 복원\n",
    "        ctx_shifted = self.ctx_expansion(ctx_shifted)\n",
    "        \n",
    "        # 프롬프트 생성\n",
    "        prompts = []\n",
    "        for ctx_shifted_i in ctx_shifted:\n",
    "            ctx_i = ctx_shifted_i.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
    "            pts_i = self.construct_prompts(ctx_i, self.token_prefix, self.token_suffix)\n",
    "            prompts.append(pts_i)\n",
    "        prompts = torch.stack(prompts)\n",
    "        \n",
    "        return prompts\n",
    "\n",
    "    def construct_prompts(self, ctx, prefix, suffix, label=None):\n",
    "        if label is not None:\n",
    "            prefix = prefix[label]\n",
    "            suffix = suffix[label]\n",
    "        \n",
    "        prompts = torch.cat([prefix, ctx, suffix], dim=1)\n",
    "        return prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9183ba47",
   "metadata": {},
   "source": [
    "[7] 데이터셋 및 데이터 로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f50ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"실험 설정\"\"\"\n",
    "    def __init__(self):\n",
    "        # 모델 설정\n",
    "        self.n_ctx = 4\n",
    "        self.n_cls = 101  # Caltech101\n",
    "        self.reduction_factor = 8  # 경량화 비율\n",
    "        \n",
    "        # 학습 설정\n",
    "        self.learning_rate = 1e-4\n",
    "        self.weight_decay = 0.01\n",
    "        self.max_epoch = 50\n",
    "        self.batch_size = 32\n",
    "        self.print_freq = 10\n",
    "        \n",
    "        # 데이터 설정\n",
    "        self.image_size = 224\n",
    "        self.train_ratio = 0.8\n",
    "\n",
    "# 데이터셋 정의\n",
    "class Caltech101Dataset(Dataset):\n",
    "    \"\"\"Caltech101 데이터셋\"\"\"\n",
    "    def __init__(self, root_dir='./data/caltech101', transform=None, train=True):\n",
    "        self.dataset = datasets.ImageFolder(root_dir, transform=transform)\n",
    "        self.classes = self.dataset.classes\n",
    "        \n",
    "        # 학습/테스트 분할 (80:20)\n",
    "        train_size = int(0.8 * len(self.dataset))\n",
    "        test_size = len(self.dataset) - train_size\n",
    "        train_dataset, test_dataset = random_split(\n",
    "            self.dataset, [train_size, test_size],\n",
    "            generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "        \n",
    "        self.data = train_dataset if train else test_dataset\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.data[idx]\n",
    "        return image, label\n",
    "\n",
    "# 데이터 전처리\n",
    "def get_transforms():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "            std=[0.26862954, 0.26130258, 0.27577711]\n",
    "        )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093657c7",
   "metadata": {},
   "source": [
    "[8] 설정 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9670b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    n_ctx = 4\n",
    "    n_cls = 101\n",
    "    reduction_factor = 8\n",
    "    learning_rate = 1e-4\n",
    "    weight_decay = 0.01\n",
    "    max_epoch = 50\n",
    "    batch_size = 32\n",
    "    print_freq = 10\n",
    "    image_size = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df7bac",
   "metadata": {},
   "source": [
    "[9] 학습 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "484e07d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name='lightweight'):\n",
    "    config = Config()\n",
    "    # ... (앞서 구현한 학습 코드)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7655f7",
   "metadata": {},
   "source": [
    "[10] 실행 및 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "598d7750",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"모델 학습을 위한 트레이너\"\"\"\n",
    "    def __init__(self, model, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model.to(self.device)\n",
    "        \n",
    "        # 옵티마이저 설정\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "        \n",
    "        # 메트릭 기록\n",
    "        self.best_acc = 0\n",
    "        self.train_losses = []\n",
    "        self.val_accuracies = []\n",
    "    \n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.model(images, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if (batch_idx + 1) % self.config.print_freq == 0:\n",
    "                print(f\"Batch [{batch_idx+1}/{len(train_loader)}] Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c83c6f",
   "metadata": {},
   "source": [
    "[11] 성능 비교 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0da0600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results():\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    # ... (결과 시각화 코드)\n",
    "    \n",
    "    \n",
    "class Trainer:\n",
    "    \"\"\"모델 학습을 위한 트레이너\"\"\"\n",
    "    def __init__(self, model, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model.to(self.device)\n",
    "        \n",
    "        # 옵티마이저 설정\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "        \n",
    "        # 메트릭 기록\n",
    "        self.best_acc = 0\n",
    "        self.train_losses = []\n",
    "        self.val_accuracies = []\n",
    "    \n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.model(images, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if (batch_idx + 1) % self.config.print_freq == 0:\n",
    "                print(f\"Batch [{batch_idx+1}/{len(train_loader)}] Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3584aec7",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b62f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, device):\n",
    "    \"\"\"개별 모델 테스트\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def run_experiment():\n",
    "    # ... (이전 코드와 동일)\n",
    "    \n",
    "    # 테스트 데이터셋 준비\n",
    "    test_dataset = Caltech101Dataset(train=False, transform=transform)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. CoOp\n",
    "    print(\"\\nTraining and Testing CoOp...\")\n",
    "    coop_model = CustomCLIP(clip_model, n_ctx=config.n_ctx, n_cls=config.n_cls)\n",
    "    coop_trainer = Trainer(coop_model, config)\n",
    "    coop_train_acc = coop_trainer.train(train_loader, val_loader, config.max_epoch)\n",
    "    coop_test_acc = test_model(coop_model, test_loader, device)\n",
    "    results['CoOp'] = {'train': coop_train_acc, 'test': coop_test_acc}\n",
    "    print(f\"CoOp Test Accuracy: {coop_test_acc:.2f}%\")\n",
    "    \n",
    "    # 2. CoCoOp\n",
    "    print(\"\\nTraining and Testing CoCoOp...\")\n",
    "    cocoop_model = CoCoOpCustomCLIP(clip_model, n_ctx=config.n_ctx, n_cls=config.n_cls)\n",
    "    cocoop_trainer = Trainer(cocoop_model, config)\n",
    "    cocoop_train_acc = cocoop_trainer.train(train_loader, val_loader, config.max_epoch)\n",
    "    cocoop_test_acc = test_model(cocoop_model, test_loader, device)\n",
    "    results['CoCoOp'] = {'train': cocoop_train_acc, 'test': cocoop_test_acc}\n",
    "    print(f\"CoCoOp Test Accuracy: {cocoop_test_acc:.2f}%\")\n",
    "    \n",
    "    # 3. Lightweight CoCoOp\n",
    "    print(\"\\nTraining and Testing Lightweight CoCoOp...\")\n",
    "    light_model = LightweightCustomCLIP(\n",
    "        clip_model, \n",
    "        n_ctx=config.n_ctx, \n",
    "        n_cls=config.n_cls,\n",
    "        reduction_factor=config.reduction_factor\n",
    "    )\n",
    "    light_trainer = Trainer(light_model, config)\n",
    "    light_train_acc = light_trainer.train(train_loader, val_loader, config.max_epoch)\n",
    "    light_test_acc = test_model(light_model, test_loader, device)\n",
    "    results['Lightweight'] = {'train': light_train_acc, 'test': light_test_acc}\n",
    "    print(f\"Lightweight CoCoOp Test Accuracy: {light_test_acc:.2f}%\")\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(\"\\nFinal Results:\")\n",
    "    for model_name, accs in results.items():\n",
    "        print(f\"{model_name}:\")\n",
    "        print(f\"  Training Accuracy: {accs['train']:.2f}%\")\n",
    "        print(f\"  Test Accuracy: {accs['test']:.2f}%\")\n",
    "    \n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
